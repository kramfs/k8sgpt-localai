# REF: https://github.com/go-skynet/helm-charts/blob/main/charts/local-ai/values.yaml
deployment:
  #image: quay.io/go-skynet/local-ai
  image:
    repository: quay.io/go-skynet/local-ai  # Example: "docker.io/myapp"
    tag: latest-aio-cpu         # latest, latest-aio-cpu
  env:
    threads: 8
    contextSize: 512
    modelsPath: "/models"

# Models to download at runtime
#models:
  # Whether to force download models even if they already exist
#  forceDownload: false

  # The list of URLs to download models from
  # Note: the name of the file will be the name of the loaded model
  #list:
  #  - url: "https://gpt4all.io/models/ggml-gpt4all-j.bin"

resources:
  {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 6000m
  #   memory: 1Gi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# Persistent storage for models and prompt templates.
# PVC and HostPath are mutually exclusive. If both are enabled, PVC configuration takes precedence.
# If neither are enabled, ephemeral storage is used.
persistence:
  models:
    enabled: true
    annotations: {}
    storageClass: ${storageClass}   # hostpath, csi
    accessModes: ReadWriteMany
    size: 10Gi
    globalMount: /models
  output:
    enabled: true
    annotations: {}
    storageClass: ${storageClass}   # hostpath, csi
    accessModes: ReadWriteMany
    size: 5Gi
    globalMount: /tmp/generated

# If using an AWS load balancer, you'll need to override the default 60s load balancer idle timeout
# service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "1200"
service:
  type: ClusterIP         # ClusterIP, LoadBalancer
  #port: 80
  annotations: {}